{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isBAYWRBbZfn"
   },
   "source": [
    "# CPU,GPU, COCO dataset - Video Analytics workshop\n",
    "\n",
    "\n",
    "\n",
    "### What is this chapter about\n",
    "In this course we will analyze an image with tensorflow. Tensordlow is open source machine learning library for research and production. If we want to understand how to process and analyze  a video file or video stream generated by camera we need to understand how a image is being processed.\n",
    "\n",
    "We are not going to create a new model from scratch (requires more time) but we will use existing and free model. You can list all  [available models based on COCO dataset](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5E7ug-1hZbOM"
   },
   "source": [
    "\n",
    "\n",
    "### COCO dataset \n",
    "[COCO dataset](http://cocodataset.org) is is formatted in JSON and it is an great object detection dataset with more than 330K images with labels. It is one of the most popular dataset and it being used in many research literatures. The datasets covers additional information such as “info”, “licenses”, “images”, “annotations”, “categories” or “segment info”.\n",
    "\n",
    "\n",
    "![alt text](http://cocodataset.org/images/coco-examples.jpg)\n",
    "\n",
    "\n",
    "### Hardware acceleration\n",
    "\n",
    "As you probably already know there is a huge difference between running VA on GPU or CPU. We will compare both approaches.\n",
    "\n",
    "*   The implementation of computing tasks in hardware to decrease latency and increase throughput is known as hardware acceleration. (wikipedia)\n",
    "*   Advantages of hardware include speedup, reduced power consumption,[1] lower latency, increased parallelism[2] and bandwidth, and better utilization of area ...  (wikipedia)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7-jY57nZeZC"
   },
   "source": [
    "\n",
    "## CPU approach\n",
    "We will start with CPU setup so make sure you have following setup in you Runtime settings.\n",
    "Click on \"Runtime\" then \"Change Runtime Type\" \n",
    "\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/VladoDemcak/accenture-va-workshop/master/images/runtime-setupv1.png =350x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "junfLMFBg69v"
   },
   "source": [
    "### Preparing for implementation\n",
    "\n",
    "As we mentioned in the beginning of the course,  tensorflow needs a model. It' a binary file we need to provide during the implementation phase. So the very first step has to be to download the files with additional files which define structure or categories. \n",
    "\n",
    "We will download  optimized tensorflow model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wL1TKsB5bZyN"
   },
   "outputs": [],
   "source": [
    "!echo \"Updating required programs...\"\n",
    "!sudo apt-get update\n",
    "!echo \"Instalation of required programs has finished!\"\n",
    "\n",
    "\n",
    "!echo \"Downloading VA prerequisites...\"\n",
    "!wget https://raw.githubusercontent.com/VladoDemcak/accenture-va-workshop/master/data/optimized_rcnn_inception_graph.pb\n",
    "!echo \"VA prerequisites have been downloaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loF6es4Xe_wh"
   },
   "source": [
    "### Implementation\n",
    "\n",
    "At this time we have all required files needed for the course.\n",
    "\n",
    "No we can focuse on analyzer itself. Everything up to this point was basically \"shell\" commands. Now we will use python code for implementing our logic.\n",
    "\n",
    "#### We will need to do following steps:\n",
    "\n",
    "\n",
    "1.   Import required dependencies\n",
    "2.   Implement helpers for doing business logic\n",
    "3.   Create dummy POJO class for representing a Detection with label and boundingbox and score\n",
    "4.   Implement analyzer itself\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8bRfHmnesHV"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import time\n",
    "import cv2\n",
    "from decimal import Decimal\n",
    "from google.protobuf import text_format\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRm-DrBgaIhQ"
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "# dont be afraid :) it's just default list of categories for default tensorflow model we have downloaded\n",
    "def categories():\n",
    "    return {\n",
    "        1: 'person',\n",
    "        2: 'bicycle',\n",
    "        3: 'car',\n",
    "        4: 'motorcycle',\n",
    "        5: 'airplane',\n",
    "        6: 'bus',\n",
    "        7: 'train',\n",
    "        8: 'truck',\n",
    "        9: 'boat',\n",
    "        10: 'traffic light',\n",
    "        11: 'fire hydrant',\n",
    "        13: 'stop sign',\n",
    "        14: 'parking meter',\n",
    "        15: 'bench',\n",
    "        16: 'bird',\n",
    "        17: 'cat',\n",
    "        18: 'dog',\n",
    "        19: 'horse',\n",
    "        20: 'sheep',\n",
    "        21: 'cow',\n",
    "        22: 'elephant',\n",
    "        23: 'bear',\n",
    "        24: 'zebra',\n",
    "        25: 'giraffe',\n",
    "        27: 'backpack',\n",
    "        28: 'umbrella',\n",
    "        31: 'handbag',\n",
    "        32: 'tie',\n",
    "        33: 'suitcase',\n",
    "        34: 'frisbee',\n",
    "        35: 'skis',\n",
    "        36: 'snowboard',\n",
    "        37: 'sports ball',\n",
    "        38: 'kite',\n",
    "        39: 'baseball bat',\n",
    "        40: 'baseball glove',\n",
    "        41: 'skateboard',\n",
    "        42: 'surfboard',\n",
    "        43: 'tennis racket',\n",
    "        44: 'bottle',\n",
    "        46: 'wine glass',\n",
    "        47: 'cup',\n",
    "        48: 'fork',\n",
    "        49: 'knife',\n",
    "        50: 'spoon',\n",
    "        51: 'bowl',\n",
    "        52: 'banana',\n",
    "        53: 'apple',\n",
    "        54: 'sandwich',\n",
    "        55: 'orange',\n",
    "        56: 'broccoli',\n",
    "        57: 'carrot',\n",
    "        58: 'hot dog',\n",
    "        59: 'pizza',\n",
    "        60: 'donut',\n",
    "        61: 'cake',\n",
    "        62: 'chair',\n",
    "        63: 'couch',\n",
    "        64: 'potted plant',\n",
    "        65: 'bed',\n",
    "        67: 'dining table',\n",
    "        70: 'toilet',\n",
    "        72: 'tv',\n",
    "        73: 'laptop',\n",
    "        74: 'mouse',\n",
    "        75: 'remote',\n",
    "        76: 'keyboard',\n",
    "        77: 'cell phone',\n",
    "        78: 'microwave',\n",
    "        79: 'oven',\n",
    "        80: 'toaster',\n",
    "        81: 'sink',\n",
    "        82: 'refrigerator',\n",
    "        84: 'book',\n",
    "        85: 'clock',\n",
    "        86: 'vase',\n",
    "        87: 'scissors',\n",
    "        88: 'teddy bear',\n",
    "        89: 'hair drier',\n",
    "        90: 'toothbrush'\n",
    "    }\n",
    "\n",
    "\n",
    "def draw_on_image(image, detections):\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    for detection in detections:\n",
    "        p1 = (int(detection.bounding_box[0] * width), int(detection.bounding_box[1] * height))\n",
    "        p2 = (int(detection.bounding_box[2] * width), int(detection.bounding_box[3] * height))\n",
    "        cv2.putText(\n",
    "            img=image,\n",
    "            text=\"{} {:.2f}\".format(detection.classification, detection.confidence),\n",
    "            org=(p1[0], p1[1] - 5),\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=1.0,\n",
    "            thickness=3,\n",
    "            color=(255, 0, 0)\n",
    "        )\n",
    "        cv2.rectangle(img=image, pt1=p1, pt2=p2, color=(255, 0, 0), thickness=2, lineType=1)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFOj60HfvAFp"
   },
   "source": [
    "For storing an attribute we will need to create a dummy POJO object without additional logic. \n",
    "Hence we need `Detection` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "716Q80AdaLAp"
   },
   "outputs": [],
   "source": [
    "#  simple POJO object as a representation of detection\n",
    "class Detection:\n",
    "\n",
    "  def __init__(self,\n",
    "               bounding_box,\n",
    "               classification,\n",
    "               confidence):\n",
    "    self.bounding_box = bounding_box\n",
    "    self.classification = classification\n",
    "    self.confidence = confidence\n",
    "\n",
    "  @staticmethod\n",
    "  def of(bbox, classification, confidence):\n",
    "    return Detection(\n",
    "        bounding_box=[Decimal(coordinate).quantize(Decimal('.0001')) for coordinate in bbox],\n",
    "        classification=classification,\n",
    "        confidence=confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhbnHUrIvTpB"
   },
   "source": [
    "But for detecting objects in an image we will need some logic. So we will create detector class with one function (`detect` function) which takes `image` as an input. \n",
    "\n",
    "With that we are able to create one instance of detector and use it multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61SlHVfkzGMe"
   },
   "outputs": [],
   "source": [
    "class TensorflowDetector:\n",
    "    model_path = \"optimized_rcnn_inception_graph.pb\"\n",
    "\n",
    "    def __init__(self, threshold, categories):\n",
    "        self.threshold = threshold\n",
    "        self.categories = categories\n",
    "\n",
    "        detection_graph = tensorflow.Graph()\n",
    "        with detection_graph.as_default():\n",
    "            od_graph_def = tensorflow.GraphDef()\n",
    "            with tensorflow.gfile.GFile(self.model_path, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "\n",
    "                # force CPU device placement for NMS ops\n",
    "                for node in od_graph_def.node:\n",
    "                    if 'BatchMultiClassNonMaxSuppression' in node.name:\n",
    "                        node.device = '/device:CPU:0'\n",
    "\n",
    "                tensorflow.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "            ops = tensorflow.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            self.tensor_dict = {}\n",
    "            for key in [\n",
    "                'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    self.tensor_dict[key] = tensorflow.get_default_graph().get_tensor_by_name(\n",
    "                        tensor_name)\n",
    "            self.image_tensor = tensorflow.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "        self.graph = detection_graph\n",
    "        with self.graph.as_default():\n",
    "            config = tensorflow.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            config.allow_soft_placement = True\n",
    "            config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "            self.sess = tensorflow.Session(config=config)\n",
    "\n",
    "    def detect(self, image):\n",
    "\n",
    "        start = time.time()\n",
    "        output_dict = self.sess.run(self.tensor_dict,\n",
    "                                    feed_dict={self.image_tensor: numpy.expand_dims(image[:, :, ::-1], 0)})\n",
    "        print(\"inference took {}s\".format(time.time() - start))\n",
    "\n",
    "        # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "        output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "        output_dict['detection_classes'] = output_dict[\n",
    "            'detection_classes'][0].astype(numpy.uint8)\n",
    "        output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "        output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "        if 'detection_masks' in output_dict:\n",
    "            output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "\n",
    "        detections = []\n",
    "        for score, label, bbox in zip(output_dict['detection_scores'], output_dict['detection_classes'],\n",
    "                                      output_dict['detection_boxes']):\n",
    "\n",
    "            if score > self.threshold:\n",
    "                ymin, xmin, ymax, xmax = [coordinate.item() for coordinate in bbox]\n",
    "                detection = Detection.of(\n",
    "                    bbox=[xmin, ymin, xmax, ymax],\n",
    "                    classification=self.categories[label.item()],\n",
    "                    confidence=score\n",
    "                )\n",
    "                detections.append(detection)\n",
    "\n",
    "        return detections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6snOHSMyiTz5"
   },
   "source": [
    "### Test inference time\n",
    "\n",
    "**Inference time** is time required for processing an image.\n",
    "\n",
    "Now, we are going to execute VA on a image. We will insert a http link to `.jpg` image and the code below will download the image from the Internet and analyze the image. \n",
    "\n",
    "Since we want to avoid anomalies (peek...) we will run the detecting several times (exactlye 10 times) in order to increase precision of our experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMcyptK2iWDl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def image_url(user_url):\n",
    "    if not user_url:\n",
    "        return \"http://images.amcnetworks.com/ifccenter.com/wp-content/uploads/2017/06/borat_1280x720.jpg\"\n",
    "    return user_url\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    detector = TensorflowDetector(\n",
    "        threshold=0.4, # everything with given confidence is fine and we will show it\n",
    "        categories=categories()\n",
    "    )\n",
    "    \n",
    "    user_url = input(\"Insert URL of an image you want to analyze and confirm with ENTER. Leave empty and default image will be used as input. \")\n",
    "    url = image_url(user_url)\n",
    "    print(\"I will use image from: {}\".format(url))\n",
    "\n",
    "    fig = plt.figure(figsize=(80, 20))\n",
    "    for i in range(0, 10):\n",
    "        resp = urlopen(url)\n",
    "        image = numpy.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        detections = detector.detect(image=image)\n",
    "        image_with_overlays = draw_on_image(image=image, detections=detections)\n",
    "        ax = fig.add_subplot(2, 5, i + 1, xticks=[], yticks=[])  # position of specific graph\n",
    "        ax.imshow(image_with_overlays)  # display image\n",
    "        ax.set_title('Iteraition #{0}'.format(i))  # label over image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2PAh2RjWcxAz"
   },
   "source": [
    "Alright. After the execution you should see output similar to following:\n",
    "\n",
    "```\n",
    "inference took 9.450029611587524s\n",
    "inference took 1.643486499786377s\n",
    "inference took 1.6418390274047852s\n",
    "inference took 1.6554217338562012s\n",
    "inference took 1.676102876663208s\n",
    "inference took 1.6452503204345703s\n",
    "inference took 1.6418006420135498s\n",
    "inference took 1.6436705589294434s\n",
    "\n",
    "```\n",
    "and of couse some images with overlays and boundingboxes.\n",
    "\n",
    "But, as we saw, the inference time is not as we want for near real-time processing. \n",
    "\n",
    "Now let's switch to a Hardware accelerator. Again click on \"Runtime\" then \"Change Runtime Type\" to GPU Hardware accelerator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFnfTAb8lG6K"
   },
   "source": [
    "\n",
    "## GPU approach\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/VladoDemcak/accenture-va-workshop/master/images/colab_options.png =350x)\n",
    "\n",
    "\n",
    " After that, run the code from previous cell and compare the output with the previous CPU experiment.\n",
    "\n",
    "The output should be similar to:\n",
    "\n",
    "```\n",
    "inference took 9.897098064422607s\n",
    "inference took 0.10806679725646973s\n",
    "inference took 0.10244631767272949s\n",
    "inference took 0.10579109191894531s\n",
    "inference took 0.10091924667358398s\n",
    "inference took 0.10539960861206055s\n",
    "inference took 0.10690879821777344s\n",
    "inference took 0.10744500160217285s\n",
    "inference took 0.10607767105102539s\n",
    "```\n",
    "\n",
    "\n",
    "### Further reading: \n",
    "*   What’s the Difference Between a CPU and a GPU? - https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/\n",
    "*   TensorFlow performance test: CPU VS GPU - https://medium.com/@andriylazorenko/tensorflow-performance-test-cpu-vs-gpu-79fcd39170c\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_hw_acceleration.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
